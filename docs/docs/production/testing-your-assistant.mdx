---
id: testing-your-assistant
sidebar_label: Testing Your Assistant
title: Testing Your Assistant
abstract:
  Rasa lets you validate and test dialogues end-to-end.
---

import useBaseUrl from "@docusaurus/useBaseUrl";

import RasaProLabel from "@theme/RasaProLabel";

import RasaProBanner from "@theme/RasaProBanner";

## Validating Data

Data validation verifies that no mistakes or major inconsistencies appear in your domain, NLU
data, or story data. To validate your data, have your CI run this command:

```bash
rasa data validate
```

If you pass a `max_history` value to one or more policies in your `config.yml` file, provide the
smallest of those values as

```bash
rasa data validate --max-history <max_history>
```

If data validation results in errors, training a model can also fail or yield bad performance, so it's
always good to run this check before training a model. By including the
`--fail-on-warnings` flag, this step will fail on warnings indicating more minor issues.

To read more about the validator and all of the available options, see [the documentation for
`rasa data validate`](../command-line-interface.mdx#rasa-data-validate).

## End-To-End Testing

<RasaProLabel />

<RasaProBanner />

:::info New in 3.5

You can now use end-to-end testing to test your assistant as a whole, including dialogue management and custom actions.

:::

End-to-end testing is an enhanced and comprehensive CLI-based testing tool that allows you to test conversation scenarios
with different pre-configured contexts, execute [custom actions](../action-server/actions.mdx), verify [response](../concepts/responses.mdx)
texts or names, and assert when [slots](../concepts/domain.mdx#slots) are filled.

End-to-end testing is not limited to testing only the NLU or the dialogue model and allows you to design
effective acceptance or integration tests. The main features of end-to-end testing are:

- integration with the [action server](../action-server/running-action-server.mdx): you can execute custom actions in your
  tests; the prerequisite is to start the action server in the background.
- test parametrization (e.g. different user profiles or other external factors): you can define multiple test fixtures
  with different pre-filled slots and re-use them in your tests.
- verifying response texts or names: you can assert that the bot response text (including [interpolated responses](../concepts/responses.mdx#using-variables-in-responses)
  with slot values and [conditional response variations](../concepts/responses.mdx#conditional-response-variations)) or `utter` name
  is as expected.
- asserting that the bot sets the slot value as expected.

### How to write test cases

To write test cases, you need to create a YAML file inside the `tests` directory of your project. The name of the file
should be `e2e_test_cases.yml`. You can also create a subdirectory inside the `tests` directory and place your test case
YAML files there. These files will be automatically discovered and run by Rasa Pro, however you need to provide
the path to the subdirectory as positional argument to the `rasa test e2e` command.

Each input file must contain the `test_cases` required key. The value of this key is a list of test cases.
Each test case must include a name given to the `test_case` key and a list of test steps given to the `steps` key.
A step can be either one of the following:

- `user`: a user message
- `bot`: a bot response
- `utter`: a domain utterance
- `slot_was_set`: a slot name and the value it was set to

You can also add the optional `fixtures` top level key if pre-filled slots are required for setting any individual
test case context. The `fixtures` key is a list of fixture names (which must be unique) and each fixture name maps to a
list of slot key-value pairs. If one of the test cases requires a pre-filled slot, you can add the fixture name to the
test case definition, by adding the fixture name to the optional `fixtures` key in the test case. The slot key-value
pairs will be set before the test case is run.

The following example shows a test case file with fixtures and two test cases that make use of all available steps:

```yaml
fixtures:
  - premium: # name of the fixture must be provided and be unique
      - membership_type: premium # every fixture can contain multiple slot key-value pairs
      - logged_in: True
  - standard:
      - logged_in: True
      - membership_type: standard

test_cases:
  - test_case: "test_premium_booking"
    fixtures:
      - premium # re-use the name of the fixture provided in fixtures section
    steps:
      - user: "Hi!"
      - bot: "Welcome back! How can I help you?"
      - user: "I want to book a trip."
      - utter: utter_ask_location
      - user: "I would like to travel to Lisbon."
      - slot_was_set:
          - location: "Lisbon"
      - utter: utter_ask_date
      - user: "I would like to travel on 22nd of June."
      - slot_was_set:
          - travel_date: "2023-06-22"
      - bot: "Great! I will book your trip to Lisbon on 22nd of June."
      - bot: "You saved 20% by being a premium member."

  - test_case: "test_anonymous_booking"
    steps:
      - user: "Hi!"
      - bot: "Hey! How can I help you?"
      - user: "I want to book a trip."
      - utter: utter_ask_location
      - user: "I would like to travel to Paris."
      - slot_was_set:
          - location: "Paris"
      - utter: utter_ask_date
      - user: "I would like to travel on 2nd of April."
      - slot_was_set:
          - travel_date: "2023-04-02"
      - bot: "Great! I will book your trip to Paris on 2nd of April."
      - bot: "You can also choose to save 20% by becoming a premium member."
```

:::note

If you are using multiple consecutive `slot_was_set` steps in your test case, the order in which these are defined must
match the order in which the slots are filled in the dialogue.

:::

### How to run the tests

To run the end-to-end tests locally or in the CI pipeline, use the [`rasa test e2e` command](../command-line-interface.mdx#rasa-test-e2e).
The command takes the following arguments:

- positional argument for the path to the test cases file or directory containing the test cases: `rasa test e2e <path>`
  If unspecified, the default path is `tests/e2e_test_cases.yml`.
- optional argument for the trained model: `--model <path>`
- optional argument for retrieving the trained model from [remote storage](./model-storage.mdx#load-model-from-cloud): `--remote-storage <remote-storage-location>`
- optional argument for the `endpoints.yml` file: `--endpoints <path>`
- optional argument for stopping the test run at first failure: `rasa test e2e --fail-fast`
- optional argument for exporting the test results to `e2e_results.yml` file: `rasa test e2e -o`

#### Testing custom actions

If the test cases include custom actions, start the action server first:

```bash
rasa run actions && rasa test e2e
```

### How to interpret the output

By default, the results are always printed to `stdout` and the command will exit with exit code `0` (if all tests passed)
or `1` (in case of test failures).

The output style is inspired by `pytest`:

- Failed test cases will be stacked, each highlighting the difference in identified mismatches in similar style to `git diff`:
  expected messages will be preceded by `+` prefix, while actual messages will be preceded by `-` prefix.
- The short test summary includes a list of every failed test case name and file location in a new line.

If `-o` flag is specified in the command, the results are also written to the `tests/e2e_results.yml` file, which will
contain a list of test results with the following keys:

- `name`: the name of the test case
- `pass_status`: the status of the test case, either `True` or `False`
- `expected_steps`: the expected test steps
- `difference`: a list of differences between the expected and actual test steps

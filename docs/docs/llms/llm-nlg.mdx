---
id: llm-nlg
sidebar_label: NLG using LLMs
title: LLMs for Natural Language Generation
abstract: |
  Enhancing your chatbot's responses using the
  advanced features of Large Language Models (LLMs). Instead of relying only on
  static response templates you can use an LLM to generate dynamic,
  context-aware responses based on templates.
---

import llmNlgExample from "./llm-nlg-example.png";
import RasaLabsLabel from "@theme/RasaLabsLabel";
import RasaLabsBanner from "@theme/RasaLabsBanner";

<RasaLabsLabel />

<RasaLabsBanner />

:::info Rasa Labs This feature is **experimental**. We introduce experimental
features to co-create with our customers. If you are interested in using this
feature, please contact us at [rasa.com/contact](https://rasa.com/contact).

The functionality might be changed or removed in the future.

:::

## Key Features

1. **Guardrails**: Using an LLM to rephrase static response templates ensures
   that the generated responses are always relevant to the conversation and
   provide more control than free-form generation.
2. **Dynamic Responses**: By employing the LLM to rephrase static response
   templates, the responses generated by your bot will sound more natural and
   conversational, enhancing user interaction.
3. **Contextual Awareness**: The LLM uses the context and previous conversation
   to rephrase the template, making the chatbot better understand and engage
   with users.
4. **Easy to Implement**: The LLM is easy to implement and can be used with
   existing chatbots without any changes to the interaction flow.

## Demo

The following example shows a demo of a chatbot using the LLM to rephrase static
response templates. Both, the right and left hand side of the conversation show
the same bot, the only difference is that the left hand side uses the LLM to
rephrase the response templates.

<Image
  img={llmNlgExample}
  caption="Bot response with and without rephrasing enabled"
  alt=""
/>

A classic: “can you order me a pizza?” can be responded to in many different
ways. The following table shows a few examples of how a bot might respond to
this question.

| response                                                                                                        | comment                                                |
| --------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ |
| I'm sorry, I can't help with that                                                                               | stilted, generic, and unimaginative                    |
| I'm sorry, I can't help you order a pizza                                                                       | makes the user feel as though they were at least heard |
| I can't help you order a pizza, delicious though it is. Do you have any questions related to your bank account? | provides a chance reinforce an assistant's personality |
| Peter, that's not the first time you've asked me for a pizza! I'll let you know if I ever learn how to do that. | personal, specific                                     |

The third and fourth examples go beyond what we could achieve with simple
templates. This is entirely achievable with a retrieval-augmented generation
approach - providing information about the user in the prompt to
help steer the LLMs generation.

:::info

The interaction flow, such as intents, slots, and actions, remains the same
whether the LLM is used or not. The LLM only modifies the responses sent back to
the user.

:::

## How to Use Rephrasing in Your Bot

Using rephrasing in your assistant requires a modification to your
`endpoints.yml` file as follows:

```yaml-rasa title="endpoints.yml"
nlg:
  type: rasa_plus.ml.LLMResponseRephraser
```

By default, rephrasing is only enabled for responses that specify
`allow_rephrasing: true` in the response template. To enable rephrasing for a
response, add this property to the responses metadata:

```yaml-rasa title="domain.yml"
responses:
  utter_greet:
    - text: "Hey! How can I help you?"
      metadata:
        rephrase: true
```

If you want to enable rephrasing for all responses, you can set the
`rephrase_all` property to `true` in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
  type: rasa_plus.ml.LLMResponseRephraser
  rephrase_all: true
```

This implementation uses a custom Natural Language Generator based on templated
responses. When a template is predicted, a prompt is created for the LLM to
rephrase the template given the current conversation. The rephrased prompt is
then returned as the response instead of the predicted template response.

## Customizing

You can customize the LLM by modifying the following parameters in the
`endpoints.yml` file.

### Rephrasing all responses

Instead of enabling rephrasing per response, you can enable it for all responses
by setting the `rephrase_all` property to `true` in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    rephrase_all: true
```

Defaults to `false`. Setting this property to `true` will enable rephrasing for
all responses, even if they don't specify `rephrase: true` in the response
metadata. If you want to disable rephrasing for a specific response, you can set
`rephrase: false` in the response metadata.

### OpenAI Model

You can specify the openai model to use for rephrasing by setting the
`model_name` property in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    model_name: text-davinci-003
```

Defaults to `text-davinci-003`. The model name needs to be set to a generative
model using the completions API of
[OpenAI](https://platform.openai.com/docs/guides/gpt/completions-api).

### Temperature

The temperature allows you to control the diversity of the generated responses.
You can specify the temperature to use for rephrasing by setting the
`temperature` property in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    temperature: 0.7
```

Defaults to `0.7`. The temperature is a value between 0 and 1 that controls the
diversity of the generated responses. Lower temperatures result in more
predictable responses, while higher temperatures result in more surprising
responses.

### Prompt

You can change the prompt used to rephrase the response by setting the `prompt`
property in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    prompt: |
        The following is a conversation with
        an AI assistant. The assistant is helpful, creative, clever, and very friendly.
        Rephrase the AI response staying close to the original message and retaining
        its meaning. Use simple english.

        Summary of the conversation:
        {{history}}

        {{current_input}}
        AI Response: {{suggested_response}}

        Rephrased Response:
```

The prompt is a [Jinja2](https://jinja.palletsprojects.com/en/3.0.x/) template
that can be used to customize the prompt. The following variables are available
in the prompt:

- `history`: The conversation history as a summary of the prior conversation,
  e.g.
  ```
  User greeted the assistant.
  ```
- `current_input`: The current user input, e.g.
  ```
  HUMAN: I want to open a bank account
  ```
- `suggested_response`: The suggested response from the LLM. e.g.
  ```
  What type of account would you like to open?
  ```

You can also customize the prompt for a single response by setting the
`rephrase_prompt` property in the response metadata:

```yaml-rasa title="domain.yml"
responses:
  utter_greet:
    - text: "Hey! How can I help you?"
      metadata:
        rephrase: true
        rephrase_prompt: |
            The following is a conversation with
            an AI assistant. The assistant is helpful, creative, clever, and very friendly.
            Rephrase the AI response staying close to the original message and retaining
            its meaning. Use simple english.

            Summary of the conversation:
            {{history}}

            {{current_input}}
            AI Response: {{suggested_response}}

            Rephrased Response:
```

## Observations

Rephrasing responses is a great way to enhance your chatbot's responses. Here
are some observations to keep in mind when using the LLM:

### Success Cases

LLM shows great potential in the following scenarios:

- **Repeated Responses**: When the bot cannot understand or process a request,
  it tends to repeat the same response. With LLMs, these repetitions sound more
  natural and less robotic.

- **General Conversation**: Overall, responses generated by LLM feel more
  conversational, even if the original templated response isn't well-crafted.
  This significantly lowers the bar for creating well-formulated responses.

### Limitations

While the LLM delivers impressive results, there are a few situations where it
may fall short:

- **Structured Responses**: If the template response contains structured
  information (e.g., bullet points), this structure might be lost during
  rephrasing. We are working on resolving this limitation of the current system.

- **Meaning Alteration**: In some cases, rephrasing might slightly change the
  original meaning of the message. We are working on addressing this issue to
  ensure consistent information delivery.

## Other documentation on LLMs

1. [Using LLMs with Rasa](./large-language-models.mdx)
2. [Setting up LLMs](./llm-setup)
3. [Intent Classification with LLMs](./llm-intent)
4. [Dialogue Handling using LLMs](./llm-intentless)
5. [Custom usage of LLMs](./llm-custom)
